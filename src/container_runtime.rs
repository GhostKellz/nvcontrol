use crate::{NvControlError, NvResult};
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::fs;
use std::process::Command;
use std::path::PathBuf;

use crate::bolt_integration::{NvControlBoltManager, GpuContainerConfig};
use tokio::runtime::Runtime;
use std::sync::Arc;
use std::sync::Mutex;

/// Pure Rust NVIDIA Container Runtime implementation
/// This provides docker/podman/bolt/nix container GPU passthrough functionality
/// built directly into nvcontrol, with native Bolt API integration

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct NvContainerRuntime {
    pub version: String,
    pub supported_runtimes: Vec<ContainerRuntime>,
    pub gpu_devices: Vec<GpuDevice>,
    pub config_path: PathBuf,
    #[serde(skip)]
    pub bolt_manager: Option<Arc<Mutex<NvControlBoltManager>>>,
}

#[derive(Debug, Clone, Serialize, Deserialize, PartialEq)]
pub enum ContainerRuntime {
    Docker,
    Podman,
    Containerd,
    Bolt,
    NixOS,
    Custom(String),
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct GpuDevice {
    pub index: u32,
    pub uuid: String,
    pub name: String,
    pub memory_mb: u64,
    pub compute_capability: (u32, u32),
    pub pci_bus_id: String,
    pub minor_number: u32,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ContainerGpuConfig {
    pub runtime: ContainerRuntime,
    pub gpu_devices: Vec<String>, // "all", "0", "1,2", "GPU-uuid"
    pub memory_limit: Option<u64>,
    pub compute_mode: String,
    pub driver_capabilities: Vec<String>, // "compute", "utility", "graphics", "video"
    pub environment_vars: HashMap<String, String>,
    pub mount_points: Vec<String>,
    pub device_requests: Vec<DeviceRequest>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct DeviceRequest {
    pub driver: String,
    pub count: Option<u32>,
    pub device_ids: Vec<String>,
    pub capabilities: Vec<Vec<String>>,
    pub options: HashMap<String, String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ContainerLaunchConfig {
    pub image: String,
    pub name: Option<String>,
    pub command: Option<Vec<String>>,
    pub working_dir: Option<String>,
    pub environment: HashMap<String, String>,
    pub volumes: Vec<VolumeMount>,
    pub ports: Vec<PortMapping>,
    pub gpu_config: ContainerGpuConfig,
    pub interactive: bool,
    pub remove_on_exit: bool,
}

#[derive(Debug, Clone, Serialize, Deserialize, PartialEq)]
pub struct VolumeMount {
    pub source: String,
    pub target: String,
    pub read_only: bool,
}

#[derive(Debug, Clone, Serialize, Deserialize, PartialEq)]
pub struct PortMapping {
    pub host_port: u16,
    pub container_port: u16,
    pub protocol: String, // "tcp" or "udp"
}

impl NvContainerRuntime {
    /// Initialize the NVIDIA container runtime with Bolt integration
    pub fn new() -> NvResult<Self> {
        let version = "1.0.0-nvcontrol".to_string();
        let gpu_devices = Self::detect_gpu_devices()?;
        let supported_runtimes = Self::detect_container_runtimes()?;

        let config_path = dirs::config_dir()
            .ok_or_else(|| NvControlError::ConfigError("No config directory".to_string()))?
            .join("nvcontrol")
            .join("container-runtime");

        fs::create_dir_all(&config_path)
            .map_err(|e| NvControlError::ConfigError(format!("Failed to create config dir: {}", e)))?;

        // Initialize Bolt manager if Bolt runtime is available
        let bolt_manager = if supported_runtimes.contains(&ContainerRuntime::Bolt) {
            let rt = Runtime::new().map_err(|e| NvControlError::RuntimeError(format!("Tokio runtime failed: {}", e)))?;
            match rt.block_on(NvControlBoltManager::new()) {
                Ok(manager) => Some(Arc::new(Mutex::new(manager))),
                Err(e) => {
                    eprintln!("Warning: Failed to initialize Bolt manager: {}", e);
                    None
                }
            }
        } else {
            None
        };

        Ok(NvContainerRuntime {
            version,
            supported_runtimes,
            gpu_devices,
            config_path,
            bolt_manager,
        })
    }

    /// Detect available GPU devices
    fn detect_gpu_devices() -> NvResult<Vec<GpuDevice>> {
        let output = Command::new("nvidia-smi")
            .args(&[
                "--query-gpu=index,uuid,name,memory.total,pci.bus_id",
                "--format=csv,noheader,nounits"
            ])
            .output()
            .map_err(|e| NvControlError::CommandFailed(format!("nvidia-smi failed: {}", e)))?;

        let mut devices = Vec::new();
        let output_str = String::from_utf8_lossy(&output.stdout);

        for line in output_str.lines() {
            let parts: Vec<&str> = line.split(", ").collect();
            if parts.len() >= 5 {
                devices.push(GpuDevice {
                    index: parts[0].parse().unwrap_or(0),
                    uuid: parts[1].to_string(),
                    name: parts[2].to_string(),
                    memory_mb: parts[3].parse().unwrap_or(0),
                    compute_capability: (0, 0), // Would need additional query
                    pci_bus_id: parts[4].to_string(),
                    minor_number: parts[0].parse().unwrap_or(0),
                });
            }
        }

        Ok(devices)
    }

    /// Detect available container runtimes
    fn detect_container_runtimes() -> NvResult<Vec<ContainerRuntime>> {
        let mut runtimes = Vec::new();

        // Check Docker
        if Command::new("docker").arg("--version").output().is_ok() {
            runtimes.push(ContainerRuntime::Docker);
        }

        // Check Podman
        if Command::new("podman").arg("--version").output().is_ok() {
            runtimes.push(ContainerRuntime::Podman);
        }

        // Check containerd
        if Command::new("containerd").arg("--version").output().is_ok() {
            runtimes.push(ContainerRuntime::Containerd);
        }

        // Check Bolt
        if Command::new("bolt").arg("--version").output().is_ok() {
            runtimes.push(ContainerRuntime::Bolt);
        }

        // Check NixOS
        if fs::metadata("/etc/nixos").is_ok() {
            runtimes.push(ContainerRuntime::NixOS);
        }

        Ok(runtimes)
    }

    /// Launch container with GPU support
    pub fn launch_container(&self, config: &ContainerLaunchConfig) -> NvResult<String> {
        match config.gpu_config.runtime {
            ContainerRuntime::Docker => self.launch_docker_container(config),
            ContainerRuntime::Podman => self.launch_podman_container(config),
            ContainerRuntime::Bolt => self.launch_bolt_container(config),
            ContainerRuntime::NixOS => self.launch_nix_container(config),
            ContainerRuntime::Containerd => self.launch_containerd_container(config),
            ContainerRuntime::Custom(ref name) => {
                Err(NvControlError::UnsupportedFeature(format!("Custom runtime {} not implemented", name)))
            }
        }
    }

    /// Launch Docker container with GPU support
    fn launch_docker_container(&self, config: &ContainerLaunchConfig) -> NvResult<String> {
        let mut cmd = Command::new("docker");
        cmd.arg("run");

        // Add GPU device requests
        if !config.gpu_config.gpu_devices.is_empty() {
            // Use --gpus flag (modern Docker)
            let gpu_spec = if config.gpu_config.gpu_devices.contains(&"all".to_string()) {
                "all".to_string()
            } else {
                format!("device={}", config.gpu_config.gpu_devices.join(","))
            };
            cmd.args(&["--gpus", &gpu_spec]);

            // Add driver capabilities
            for cap in &config.gpu_config.driver_capabilities {
                cmd.env("NVIDIA_DRIVER_CAPABILITIES", cap);
            }
        }

        // Add environment variables
        for (key, value) in &config.environment {
            cmd.args(&["-e", &format!("{}={}", key, value)]);
        }

        // Add volume mounts
        for volume in &config.volumes {
            let mount_spec = if volume.read_only {
                format!("{}:{}:ro", volume.source, volume.target)
            } else {
                format!("{}:{}", volume.source, volume.target)
            };
            cmd.args(&["-v", &mount_spec]);
        }

        // Add port mappings
        for port in &config.ports {
            cmd.args(&["-p", &format!("{}:{}:{}", port.host_port, port.container_port, port.protocol)]);
        }

        // Container name
        if let Some(ref name) = config.name {
            cmd.args(&["--name", name]);
        }

        // Interactive mode
        if config.interactive {
            cmd.args(&["-it"]);
        }

        // Remove on exit
        if config.remove_on_exit {
            cmd.arg("--rm");
        }

        // Working directory
        if let Some(ref workdir) = config.working_dir {
            cmd.args(&["-w", workdir]);
        }

        // Image
        cmd.arg(&config.image);

        // Command
        if let Some(ref command) = config.command {
            cmd.args(command);
        }

        // Execute command
        let output = cmd.output()
            .map_err(|e| NvControlError::CommandFailed(format!("Docker run failed: {}", e)))?;

        if output.status.success() {
            let container_id = String::from_utf8_lossy(&output.stdout).trim().to_string();
            Ok(container_id)
        } else {
            let error = String::from_utf8_lossy(&output.stderr);
            Err(NvControlError::CommandFailed(format!("Docker run failed: {}", error)))
        }
    }

    /// Launch Podman container with GPU support
    fn launch_podman_container(&self, config: &ContainerLaunchConfig) -> NvResult<String> {
        let mut cmd = Command::new("podman");
        cmd.arg("run");

        // Add GPU device access for Podman
        if !config.gpu_config.gpu_devices.is_empty() {
            // Podman uses --device for GPU access
            for gpu_device in &self.gpu_devices {
                cmd.args(&["--device", &format!("/dev/nvidia{}", gpu_device.index)]);
                cmd.args(&["--device", "/dev/nvidiactl"]);
                cmd.args(&["--device", "/dev/nvidia-modeset"]);
                cmd.args(&["--device", "/dev/nvidia-uvm"]);
                cmd.args(&["--device", "/dev/nvidia-uvm-tools"]);
            }

            // Add NVIDIA environment variables
            cmd.args(&["-e", "NVIDIA_VISIBLE_DEVICES=all"]);
            cmd.args(&["-e", &format!("NVIDIA_DRIVER_CAPABILITIES={}",
                config.gpu_config.driver_capabilities.join(","))]);
        }

        // Add environment variables
        for (key, value) in &config.environment {
            cmd.args(&["-e", &format!("{}={}", key, value)]);
        }

        // Add volume mounts
        for volume in &config.volumes {
            let mount_spec = if volume.read_only {
                format!("{}:{}:ro", volume.source, volume.target)
            } else {
                format!("{}:{}", volume.source, volume.target)
            };
            cmd.args(&["-v", &mount_spec]);
        }

        // Add port mappings
        for port in &config.ports {
            cmd.args(&["-p", &format!("{}:{}", port.host_port, port.container_port)]);
        }

        // Container name
        if let Some(ref name) = config.name {
            cmd.args(&["--name", name]);
        }

        // Interactive mode
        if config.interactive {
            cmd.args(&["-it"]);
        }

        // Remove on exit
        if config.remove_on_exit {
            cmd.arg("--rm");
        }

        // Working directory
        if let Some(ref workdir) = config.working_dir {
            cmd.args(&["-w", workdir]);
        }

        // Image
        cmd.arg(&config.image);

        // Command
        if let Some(ref command) = config.command {
            cmd.args(command);
        }

        // Execute command
        let output = cmd.output()
            .map_err(|e| NvControlError::CommandFailed(format!("Podman run failed: {}", e)))?;

        if output.status.success() {
            let container_id = String::from_utf8_lossy(&output.stdout).trim().to_string();
            Ok(container_id)
        } else {
            let error = String::from_utf8_lossy(&output.stderr);
            Err(NvControlError::CommandFailed(format!("Podman run failed: {}", error)))
        }
    }

    /// Launch Bolt container with GPU support using native API
    fn launch_bolt_container(&self, config: &ContainerLaunchConfig) -> NvResult<String> {
        if let Some(ref bolt_manager) = self.bolt_manager {
            let manager = bolt_manager.lock()
                .map_err(|e| NvControlError::RuntimeError(format!("Bolt manager lock failed: {}", e)))?;

            // Convert container launch config to Bolt GPU config
            let gpu_config = self.create_bolt_gpu_config(config)?;

            let rt = Runtime::new()
                .map_err(|e| NvControlError::RuntimeError(format!("Tokio runtime failed: {}", e)))?;

            let workload_name = config.name.as_deref().unwrap_or("nvcontrol-workload");

            match rt.block_on(manager.launch_gpu_workload(
                workload_name,
                &config.image,
                &gpu_config,
            )) {
                Ok(container_name) => Ok(container_name),
                Err(e) => Err(NvControlError::CommandFailed(format!("Bolt launch failed: {}", e)))
            }
        } else {
            // Fallback to CLI approach
            self.launch_bolt_container_cli(config)
        }
    }

    /// Create Bolt GPU configuration from container launch config
    fn create_bolt_gpu_config(&self, config: &ContainerLaunchConfig) -> NvResult<GpuContainerConfig> {
        let gpu_id = if config.gpu_config.gpu_devices.is_empty() {
            0
        } else {
            config.gpu_config.gpu_devices[0]
                .parse()
                .unwrap_or(0)
        };

        Ok(GpuContainerConfig {
            gpu_id,
            memory_limit: config.gpu_config.memory_limit,
            compute_capabilities: config.gpu_config.driver_capabilities.clone(),
            power_limit: None, // Could be extracted from environment
            enable_dlss: config.gpu_config.driver_capabilities.contains(&"graphics".to_string()),
            enable_raytracing: config.gpu_config.driver_capabilities.contains(&"graphics".to_string()),
            enable_cuda: config.gpu_config.driver_capabilities.contains(&"compute".to_string()),
        })
    }

    /// Fallback CLI launch for Bolt containers
    fn launch_bolt_container_cli(&self, config: &ContainerLaunchConfig) -> NvResult<String> {
        // Generate Boltfile configuration for this container
        let boltfile_config = self.generate_boltfile_config(config)?;

        // Create temporary Boltfile
        let boltfile_path = format!("/tmp/nvcontrol-boltfile-{}.toml",
            std::time::SystemTime::now().duration_since(std::time::UNIX_EPOCH)
                .unwrap_or_default().as_secs());
        fs::write(&boltfile_path, boltfile_config)
            .map_err(|e| NvControlError::ConfigError(format!("Failed to write Boltfile: {}", e)))?;

        // Use Bolt CLI
        let output = Command::new("bolt")
            .args(&["surge", "up", "--config", &boltfile_path])
            .output()
            .map_err(|e| NvControlError::CommandFailed(format!("Bolt surge up failed: {}", e)))?;

        // Clean up temporary Boltfile
        let _ = fs::remove_file(boltfile_path);

        if output.status.success() {
            let container_id = String::from_utf8_lossy(&output.stdout).trim().to_string();
            Ok(container_id)
        } else {
            let error = String::from_utf8_lossy(&output.stderr);
            Err(NvControlError::CommandFailed(format!("Bolt surge up failed: {}", error)))
        }
    }

    /// Generate Boltfile configuration for container launch
    fn generate_boltfile_config(&self, config: &ContainerLaunchConfig) -> NvResult<String> {
        let service_name = config.name.as_deref()
            .unwrap_or_else(|| config.image.split(':').next().unwrap_or("gpu-service"));

        let mut boltfile = format!(r#"project = "nvcontrol-runtime"

[services.{}]
build = "{}"
"#, service_name, config.image);

        // Add GPU configuration
        if !config.gpu_config.gpu_devices.is_empty() {
            boltfile.push_str("privileged = true\n");
            boltfile.push_str("gpu.nvidia = true\n");

            // Add GPU devices
            boltfile.push_str("devices = [\n");
            for device in &self.gpu_devices {
                boltfile.push_str(&format!("    \"/dev/nvidia{}\",\n", device.index));
            }
            boltfile.push_str("    \"/dev/nvidia-uvm\",\n");
            boltfile.push_str("    \"/dev/nvidia-modeset\",\n");
            boltfile.push_str("    \"/dev/nvidiactl\"\n");
            boltfile.push_str("]\n");

            // Add NVIDIA library volumes
            boltfile.push_str("volumes = [\n");
            boltfile.push_str("    \"/usr/lib/x86_64-linux-gnu/libnvidia-ml.so:/usr/lib/x86_64-linux-gnu/libnvidia-ml.so:ro\",\n");
            boltfile.push_str("    \"/sys/class/drm:/sys/class/drm:ro\"\n");
            boltfile.push_str("]\n");

            // Add NVIDIA environment variables
            boltfile.push_str("env.NVIDIA_VISIBLE_DEVICES = \"all\"\n");
            boltfile.push_str(&format!("env.NVIDIA_DRIVER_CAPABILITIES = \"{}\"\n",
                config.gpu_config.driver_capabilities.join(",")));
        }

        // Add custom environment variables
        for (key, value) in &config.environment {
            boltfile.push_str(&format!("env.{} = \"{}\"\n", key, value));
        }

        // Add custom volume mounts
        if !config.volumes.is_empty() {
            if !boltfile.contains("volumes = [") {
                boltfile.push_str("volumes = [\n");
            } else {
                // Add to existing volumes array
                boltfile = boltfile.replace("]\n", "");
            }
            for volume in &config.volumes {
                let ro_suffix = if volume.read_only { ":ro" } else { "" };
                boltfile.push_str(&format!("    \"{}:{}{}\"", volume.source, volume.target, ro_suffix));
                if volume != config.volumes.last().unwrap() {
                    boltfile.push_str(",");
                }
                boltfile.push_str("\n");
            }
            boltfile.push_str("]\n");
        }

        // Add port mappings
        if !config.ports.is_empty() {
            boltfile.push_str("ports = [\n");
            for port in &config.ports {
                boltfile.push_str(&format!("    \"{}:{}\"", port.host_port, port.container_port));
                if port != config.ports.last().unwrap() {
                    boltfile.push_str(",");
                }
                boltfile.push_str("\n");
            }
            boltfile.push_str("]\n");
        }

        // Add working directory
        if let Some(ref workdir) = config.working_dir {
            boltfile.push_str(&format!("workdir = \"{}\"\n", workdir));
        }

        // Add network configuration for encrypted QUIC
        boltfile.push_str("\n[network]\n");
        boltfile.push_str("driver = \"quic\"\n");
        boltfile.push_str("encryption = true\n");

        Ok(boltfile)
    }

    /// Launch NixOS container with GPU support
    fn launch_nix_container(&self, config: &ContainerLaunchConfig) -> NvResult<String> {
        // Generate NixOS configuration for GPU container
        let nix_config = self.generate_nix_gpu_config(config)?;

        // Write configuration to temporary file
        let temp_config = format!("/tmp/nvcontrol-{}.nix", chrono::Utc::now().timestamp());
        fs::write(&temp_config, nix_config)
            .map_err(|e| NvControlError::ConfigError(format!("Failed to write Nix config: {}", e)))?;

        // Launch container using nixos-container
        let output = Command::new("nixos-container")
            .args(&["create", "--config", &temp_config])
            .output()
            .map_err(|e| NvControlError::CommandFailed(format!("NixOS container failed: {}", e)))?;

        // Clean up temp file
        let _ = fs::remove_file(temp_config);

        if output.status.success() {
            let container_name = String::from_utf8_lossy(&output.stdout).trim().to_string();

            // Start the container
            Command::new("nixos-container")
                .args(&["start", &container_name])
                .output()
                .map_err(|e| NvControlError::CommandFailed(format!("Failed to start NixOS container: {}", e)))?;

            Ok(container_name)
        } else {
            let error = String::from_utf8_lossy(&output.stderr);
            Err(NvControlError::CommandFailed(format!("NixOS container creation failed: {}", error)))
        }
    }

    /// Generate NixOS configuration for GPU container
    fn generate_nix_gpu_config(&self, config: &ContainerLaunchConfig) -> NvResult<String> {
        let mut nix_config = String::from(r#"
{ config, pkgs, ... }:
{
  # NVIDIA GPU support in NixOS container
  hardware.opengl = {
    enable = true;
    driSupport = true;
    driSupport32Bit = true;
  };

  services.xserver.videoDrivers = [ "nvidia" ];

  hardware.nvidia = {
    modesetting.enable = true;
    powerManagement.enable = false;
    powerManagement.finegrained = false;
    open = true; # Use nvidia-open drivers
    nvidiaSettings = true;
  };

  # Container-specific environment
  environment.systemPackages = with pkgs; [
    nvidia-docker
    docker
    git
  ];
"#);

        // Add environment variables
        if !config.environment.is_empty() {
            nix_config.push_str("\n  environment.variables = {\n");
            for (key, value) in &config.environment {
                nix_config.push_str(&format!("    {} = \"{}\";\n", key, value));
            }
            nix_config.push_str("  };\n");
        }

        // Add GPU-specific configuration
        if !config.gpu_config.gpu_devices.is_empty() {
            nix_config.push_str(r#"

  # GPU device access
  boot.extraModprobeConfig = ''
    options nvidia-drm modeset=1
  '';

  # Container runtime configuration
  virtualisation.docker = {
    enable = true;
    enableNvidia = true;
  };
"#);
        }

        nix_config.push_str("\n}\n");
        Ok(nix_config)
    }

    /// Launch containerd container (for Kubernetes/cri-o)
    fn launch_containerd_container(&self, config: &ContainerLaunchConfig) -> NvResult<String> {
        // Generate container configuration
        let container_config = self.generate_containerd_config(config)?;
        let config_path = format!("/tmp/nvcontrol-container-{}.json", chrono::Utc::now().timestamp());

        fs::write(&config_path, container_config)
            .map_err(|e| NvControlError::ConfigError(format!("Failed to write container config: {}", e)))?;

        // Use ctr (containerd CLI) to run container
        let output = Command::new("ctr")
            .args(&[
                "containers",
                "create",
                "--config", &config_path,
                &config.image,
                &config.name.as_deref().unwrap_or("nvcontrol-container"),
            ])
            .output()
            .map_err(|e| NvControlError::CommandFailed(format!("containerd create failed: {}", e)))?;

        // Clean up config file
        let _ = fs::remove_file(config_path);

        if output.status.success() {
            let container_id = String::from_utf8_lossy(&output.stdout).trim().to_string();
            Ok(container_id)
        } else {
            let error = String::from_utf8_lossy(&output.stderr);
            Err(NvControlError::CommandFailed(format!("containerd failed: {}", error)))
        }
    }

    /// Generate containerd container configuration
    fn generate_containerd_config(&self, config: &ContainerLaunchConfig) -> NvResult<String> {
        #[derive(Serialize)]
        struct ContainerdConfig {
            image: String,
            env: Vec<String>,
            working_dir: Option<String>,
            command: Option<Vec<String>>,
        }

        let mut env_vars = Vec::new();

        // Add GPU environment variables
        if !config.gpu_config.gpu_devices.is_empty() {
            env_vars.push("NVIDIA_VISIBLE_DEVICES=all".to_string());
            env_vars.push(format!("NVIDIA_DRIVER_CAPABILITIES={}",
                config.gpu_config.driver_capabilities.join(",")));
        }

        // Add custom environment variables
        for (key, value) in &config.environment {
            env_vars.push(format!("{}={}", key, value));
        }

        let containerd_config = ContainerdConfig {
            image: config.image.clone(),
            env: env_vars,
            working_dir: config.working_dir.clone(),
            command: config.command.clone(),
        };

        serde_json::to_string_pretty(&containerd_config)
            .map_err(|e| NvControlError::ConfigError(format!("JSON serialization failed: {}", e)))
    }

    /// Monitor running GPU containers
    pub fn monitor_gpu_containers(&self) -> NvResult<Vec<super::container::ContainerGpuInfo>> {
        let mut all_containers = Vec::new();

        // Monitor Docker containers
        if self.supported_runtimes.contains(&ContainerRuntime::Docker) {
            if let Ok(mut containers) = self.get_docker_gpu_containers() {
                all_containers.append(&mut containers);
            }
        }

        // Monitor Podman containers
        if self.supported_runtimes.contains(&ContainerRuntime::Podman) {
            if let Ok(mut containers) = self.get_podman_gpu_containers() {
                all_containers.append(&mut containers);
            }
        }

        Ok(all_containers)
    }

    /// Get Docker GPU containers
    fn get_docker_gpu_containers(&self) -> NvResult<Vec<super::container::ContainerGpuInfo>> {
        super::container::list_gpu_containers()
    }

    /// Get Podman GPU containers
    fn get_podman_gpu_containers(&self) -> NvResult<Vec<super::container::ContainerGpuInfo>> {
        let output = Command::new("podman")
            .args(&["ps", "--format", "json"])
            .output()
            .map_err(|e| NvControlError::CommandFailed(format!("Podman ps failed: {}", e)))?;

        let _containers_json = String::from_utf8_lossy(&output.stdout);
        // Parse JSON and filter for GPU containers
        // This would need proper JSON parsing in production

        Ok(Vec::new()) // Placeholder
    }

    /// Create PhantomLink audio container configuration
    pub fn create_phantomlink_container_config(&self) -> NvResult<ContainerLaunchConfig> {
        Ok(ContainerLaunchConfig {
            image: "ghcr.io/ghostkellz/phantomlink:latest".to_string(),
            name: Some("phantomlink-audio".to_string()),
            command: None,
            working_dir: Some("/app".to_string()),
            environment: HashMap::from([
                ("RUST_LOG".to_string(), "info".to_string()),
                ("PULSEAUDIO_SERVER".to_string(), "unix:/run/user/1000/pulse/native".to_string()),
                ("ALSA_CARD".to_string(), "0".to_string()),
                ("RTX_VOICE_ENABLED".to_string(), "true".to_string()),
            ]),
            volumes: vec![
                VolumeMount {
                    source: "/run/user/1000/pulse".to_string(),
                    target: "/run/user/1000/pulse".to_string(),
                    read_only: false,
                },
                VolumeMount {
                    source: "/dev/snd".to_string(),
                    target: "/dev/snd".to_string(),
                    read_only: false,
                },
            ],
            ports: vec![
                PortMapping {
                    host_port: 8080,
                    container_port: 8080,
                    protocol: "tcp".to_string(),
                },
            ],
            gpu_config: ContainerGpuConfig {
                runtime: ContainerRuntime::Docker,
                gpu_devices: vec!["all".to_string()],
                memory_limit: Some(2 * 1024 * 1024 * 1024), // 2GB
                compute_mode: "default".to_string(),
                driver_capabilities: vec!["compute".to_string(), "utility".to_string()],
                environment_vars: HashMap::from([
                    ("NVIDIA_VISIBLE_DEVICES".to_string(), "all".to_string()),
                    ("NVIDIA_DRIVER_CAPABILITIES".to_string(), "compute,utility".to_string()),
                ]),
                mount_points: vec![
                    "/usr/local/nvidia".to_string(),
                ],
                device_requests: vec![
                    DeviceRequest {
                        driver: "nvidia".to_string(),
                        count: Some(1),
                        device_ids: vec!["0".to_string()],
                        capabilities: vec![vec!["gpu".to_string()]],
                        options: HashMap::new(),
                    }
                ],
            },
            interactive: false,
            remove_on_exit: false,
        })
    }
}

impl Default for ContainerGpuConfig {
    fn default() -> Self {
        Self {
            runtime: ContainerRuntime::Docker,
            gpu_devices: vec!["0".to_string()],
            memory_limit: None,
            compute_mode: "default".to_string(),
            driver_capabilities: vec!["compute".to_string(), "utility".to_string()],
            environment_vars: HashMap::new(),
            mount_points: Vec::new(),
            device_requests: Vec::new(),
        }
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_runtime_detection() {
        let runtime = NvContainerRuntime::new();
        assert!(runtime.is_ok() || runtime.is_err()); // Just ensure it doesn't panic
    }

    #[test]
    fn test_phantomlink_config_creation() {
        let runtime = NvContainerRuntime::new().unwrap();
        let config = runtime.create_phantomlink_container_config().unwrap();
        assert_eq!(config.image, "ghcr.io/ghostkellz/phantomlink:latest");
        assert!(config.environment.contains_key("RTX_VOICE_ENABLED"));
    }
}